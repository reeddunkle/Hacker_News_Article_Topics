[u'Today, the federal appeals court for Washington, DC\xa0upheld the legal authority behind FCC\'s Open Internet Order in a 2-1 decision,\xa0as first reported by Politico. Faced with multiple legal challenges, the court declined to pare back any of the commission\'s powers. The ruling can still be appealed to the Supreme Court, but it hands a major victory to FCC chairman Tom Wheeler and net neutrality advocates at large.\n\nFiled the day after the FCC issued its landmark Open Internet Order, the lawsuit challenges the FCC\'s power to classify internet providers as common carriers under Title II. "These rules will undermine future investment by large and small broadband providers, to the detriment of consumers," The National Cable and Telecommunications Association argued in a brief in support of the lawsuit. "[The order] willfully ignores the hundreds of billions of dollars invested in reliance on the prior policy."\n\n"Today\u2019s ruling is a victory for consumers and innovators"\n\nThe lawsuit also questioned whether the FCC had the authority to group wired and wireless services under the same rules. During oral arguments, judges had expressed support for the idea of extending the rules to wireless services. "So if I\u2019m walking in my house with an iPad," Judge Srinivasan asked a lawyer, "at one end of the hall I connect to my Wi-Fi, at the other end, my device switches over to my wireless subscription \u2014 did Congress really intend these two services to be regulated totally differently even if I can\u2019t tell the difference?"\n\nThe DC Appeals court has traditionally been a tricky venue for the FCC, striking down a number of\xa0previous net neutrality measures that did not rely on Title II classification and leading to widespread uncertainty as to how the court would rule. In this case, however, the FCC\'s case prevailed.\n\nFCC chairman Tom Wheeler has already applauded the ruling. "Today\u2019s ruling is a victory for consumers and innovators who deserve unfettered access to the entire web," Wheeler said in a statement. "After a decade of debate and legal battles, today\u2019s ruling affirms the Commission\u2019s ability to enforce the strongest possible internet protections \u2014 both on fixed and mobile networks \u2014 that will ensure the internet remains open, now and in the future."\n\nNotably, the Open Internet Order does not affect zero-rating services like T-Mobile\'s BingeOn or Verizon\'s Go90, which are intentionally left out of the scope of the order. "I can argue there are some aspects of [zero rating] that are good, and I can argue there\u2019s some aspects of it that are not so good," Wheeler told The Verge in an interview in March. "The job of the regulator is to figure out, \'Okay, now how do I deal with this?\'"',
 u'U.S. Appeals Court Holds Up Net Neutrality Rules In Full\n\nA federal appeals court on Tuesday fully upheld the so-called Open Internet rules, regulations backing the principle of net neutrality.\n\nIt\'s the idea that phone and cable companies should treat all of the traffic on their networks equally \u2014 no blocking or slowing their competitors, and no fast lanes for companies that can pay more.\n\nThe U.S. Court of Appeals for the D.C. Circuit rejected the petition filed by telecom, cable and wireless industry associations alongside AT&T, CenturyLink and several smaller providers. President Obama and various Internet and venture companies and public interest groups backed the Federal Communications Commission.\n\nThis was the third time in less than a decade that the court has tackled the FCC\'s attempts at regulating Internet service providers.\n\nAs we explained before, the key question before the court was whether the FCC had proper authority to reclassify broadband Internet as a more heavily regulated telecommunications service, similar to traditional telephony. As we have reported:\n\n"This court, early (in 2014), threw out the FCC\'s earlier rules because it ruled they were effectively treating ISPs as if they were providing \'telecommunications services,\' sort of like traditional telephone companies, even though the FCC itself had classified them in the more lightly regulated category of \'information service providers.\' So this time around, the FCC (with President Obama\'s weigh-in) decided to reclassify broadband Internet access under what\'s known as Title II of the Telecommunications Act \u2014 considering it like an essential public utility, like landline telephone service in the past century."',
 u"The use of Graphics Processing Units for rendering is well known, but their power for general parallel computation has only recently been explored. Parallel algorithms running on GPUs can often achieve up to 100x speedup over similar CPU algorithms, with many existing applications for physics simulations, signal processing, financial modeling, neural networks, and countless other fields.\n\nThis course will cover programming techniques for the GPU. The course will introduce NVIDIA's parallel computing language, CUDA. Beyond covering the CUDA programming model and syntax, the course will also discuss GPU architecture, high performance computing on GPUs, parallel algorithms, CUDA libraries, and applications of GPU computing. Problem sets will cover performance optimization and specific GPU applications in numerical mathematics, medical imaging, finance, and other fields.\n\nLabwork will require significant programming. A working knowledge of the C programming language will be necessary. Although CS 24 is not a prerequisite, it (or equivalent systems programming experience) is recommended.",
 u'',
 u'Please OS X (or whatever your name is) just fucking reset your DNS cache please',
 u"Software Freedom Conservancy congratulates its phpMyAdmin project on succesfuly completing completing a thorough security audit, as part of Mozilla's Secure Open Source Fund. No serious issues were found in the phyMyAdmin codebase.\n\nMozilla launched the SOS Fund as part of its Open Source Support Program. The SOS Fund focuses on auditing, remediation, and verification for key open source software projects. Conservancy's phpMyAdmin project was one of the first projects selected for the program. The security audit was performed by NCC Group. The phpMyAdmin team participated actively in the audit, making its key members available to the NCC Group team. As the audit states, the project has been one of the defacto tools for managing and maintaining MySQL databases for years. Its wide adoption matched with its potential for misuse, warrants regular review from a security perspective.\n\nWhile no serious issues were found, the audit team found 3 medium risk and 5 low risk vulnerabilities, plus one informational issue. Most of these issues are already fixed in 4.6.2 release, and the more severe issues were covered by PMASA-2016-14, PMASA-2016-15 and PMASA-2016-16. The fixes were backported to older releases as well.\n\nWe at the phpMyAdmin project are excited to have been one of the early programs selected by the Mozilla SOS Fund, said project team member Isaac Bennetch, We appreciate Mozilla's dedication to ensuring making software more secure and are pleased that no serious flaws were found during the phpMyAdmin audit.\n\nConservancy and the phpMyAdmin project are proud of the results and thank Mozilla for funding and initiating the audit, well positioning phpMyAdmin to continue its critical role in free software with confidence. The full audit report is available here.",
 u'',
 u'Last.fm is a web site\xa0that tracks your music listening history across devices (computer, phone, iPod, etc) and services (Spotify, iTunes, Google Play, etc). I\u2019ve been using Last.fm for nearly 10 years now, and my tracked listening history goes back even further when you consider all my pre-existing iTunes play counts that I scrobbled\xa0(ie, submitted\xa0to my Last.fm database) when I joined Last.fm.\n\nUsing\xa0Python, pandas, matplotlib, and leaflet, I downloaded my listening history from Last.fm\u2019s API, analyzed and visualized the data, downloaded full artist details from the Musicbrainz API, then geocoded and mapped all the artists I\u2019ve played. All of my code used to do this\xa0is available in this GitHub repo, and is\xa0easy to re-purpose for exploring your own Last.fm history. All you need is an API key.\n\nFirst I visualized\xa0my most-played artists, above. Across the dataset, I have 263,428 scrobbles (aka,\xa0song\xa0plays). I\u2019ve listened to 15,503 different artists and 53,632 different songs across\xa025,804 different albums from when I first started using iTunes circa 2005 through the present day. This includes pretty close to every song I\u2019ve played on anything other than vinyl during that time.\n\nI also mapped all the artists I\u2019ve listened to. To do this, I took each artist ID\xa0in the Last.fm data set and passed it to the Musicbrainz API to get full artist details. Then I recursively queried the place until I got a full place name, like \u201cBrixton, London, England, UK\u201d (this process takes a while, and is perfectly suited\xa0to run on a Raspberry Pi!). Next\xa0I geocoded these place names to latitude-longitude using the Nominatim and Google APIs. Finally I mapped these points in Python with matplotlib\xa0basemap:\n\nI also converted\xa0these\xa0points to GeoJSON to produce an\xa0interactive Leaflet web map of the artists I listen to (see this previous post for more on exporting pandas DataFrames to GeoJSON). Click any point in the map below to see a list of artists from there:\n\nI predominately listen to artists from\xa0the populated areas of the U.S. and the blue banana. But, this map is not fully representative of all the artists I\u2019ve played, because many of them lack a Musicbrainz ID on Last.fm, and many who do have an ID lack place information in the Musicbrainz database. This database also over-represents\xa0Western artists and artists listened to by Westerners, and under-represents other artists around the world (that appear in my listening history).\n\nI was curious about my most-played artists\u2019 relative performance over time. So, I took the top six artists and charted their cumulative play counts since 2009:\n\nDavid Bowie is the big winner here, moving from sixth place in 2009 all the way up to first place today\xa0as my most-played artist (since Last.fm sign-up). Note that these disaggregate data differ slightly from the aggregate play counts per artist earlier, because Last.fm here discards the lump-sum play counts from iTunes that I first scrobbled upon signing up. The disaggregate scrobbles with dates thus only represent post-sign-up song plays.\n\nI wanted to look more into\xa0these time dynamics of my listening history. How have they\xa0changed over the years? And, when exactly do\xa0I spend time listening to music? First I looked at my songs played\xa0per month since January 2010:\n\nAlthough there are a couple of big spikes, during most months I listen to somewhere around 1,000 to 2,500 songs. The peak during March 2015 coincides with my doctoral qualifying exams, which saw me sitting in my room about 16 hours day reading, writing\u2026 and listening to music. Next I looked at which days of the week I do most of my listening:\n\nSo, I listen to the most music on Fridays, and the least on Saturdays. The weekdays are all consistently higher as I tend to listen to music all day long while I\u2019m working. The weekends are consistently lower as I tend to be out and about more, away from my computer and\xa0stereo. Next I looked at my cumulative listening history\xa0by hour of the day:\n\nThis chart essentially follows my sleep, wake, work schedule. Most of my listening occurs during the mid-day while I\u2019m working and tails off into the evening. But this aggregate pattern isn\u2019t exactly same each day. Here I broke out the hourly chart above, by\xa0each day of the week:\n\nNow it\u2019s easy to see the low days of Saturday and Sunday \u2013 but interestingly, Saturday has\xa0my highest play count late at night, when I\u2019m up late PARTYING. Fridays at noon and Wednesdays at 3pm have the highest peaks. Also, my Monday mornings get off to slower starts than the rest of the work days, as I recover from the weekend.\n\nFor yuks, I looked at a couple traits of artist names. The first is the frequency of artist names beginning with each letter of the alphabet (sans a preceding \u201cthe\u201d):\n\nS\u2019s and M\u2019s lead the pack, and Q\u2019s and X\u2019s bring\xa0up the rear. Next\xa0I looked at the frequency of artist name lengths:\n\nNot everyone can be a name length outlier like\xa0X\xa0and\xa0Orchestral Manoeuvres in the Dark.\n\nFinally, I\u2019ll wrap this up similarly to how I started it by visualizing my most-played songs and albums of all-time on Last.fm. First, my most-played\xa0tracks:\n\nThere are some common themes here: similar artists appear in both the most-played songs and most-played albums lists, unsurprisingly. There\u2019s also a pretty clear correlation between the most-played albums and the number of tracks on the album, as these data are\xa0not normalized by the latter. It\xa0might be possible to normalize album play counts by number of tracks on the album, by querying the MusicBrainz API for more information.\n\nTo recap, I downloaded my listening history from Last.fm, analyzed and visualized the data, downloaded full artist details from the Musicbrainz API, then geocoded and mapped all the artists I\u2019ve played, and finally dumped these points to GeoJSON for leaflet web mapping. All of my Python and leaflet code used to do this\xa0is available in this GitHub repo, and is\xa0easy to re-purpose for exploring your own Last.fm history.\n\nYou might also be interested in:',
 u"Habitat is a new approach to automation that focuses on the application instead of the infrastructure it runs on. With Habitat, the apps you build, deploy and manage behave consistently in any runtime\u2009\u2014\u2009metal, VMs, containers, and PaaS. You'll spend less time on the environment and more time building features.\n\nHabitat is an open source project, and we\u2019d love for you to get involved.\n\nTake a quick, interactive tour of Habitat features.\n\n Just need the bits? Download Habitat.",
 u'The Checked C research project is investigating how to extend the C programming language so that programmers can write more secure and reliable C programs. The project is developing an extension to C called Checked C that adds checking to C to detect or prevent common programming errors such as buffer overruns, out-of-bounds memory accesses, and incorrect type casts.\n\nThe latest version of the Checked C specification is available here. The Checked C repository has the LaTex files for the specification, as well as tests for the language extension tests. We are currently implementing Checked C in LLVM/clang. The Checked C clang repository and the Checked C LLVM repository have the source code for the compiler implementation.\n\nChecked C is an open, collaborative research project. Researchers and developers can contribute to the specification and the compiler implementation on Github. We are working with Michael Hicks and Andrew Ruef\xa0at the University of Maryland on Checked C.\xa0 We also\xa0have received feedback from researchers at Samsung and Cornell.\n\nMost system software is written in C or C++, which is based on C. System software includes operating systems, browsers, databases, and programming language interpreters. System software is the \u201cinfrastructure\u201d software that the world runs on.\n\nThere are certain kinds of programming errors such as buffer overruns and incorrect type casts that programmers can make when writing C or C++ programs. These errors can lead to security vulnerabilities or software reliability problems. The Checked C extension will let programmers add checking to their programs to detect these kinds of errors when a program runs or while it is being written. Existing system software can be modified incrementally in a backwards-compatible fashion to have this checking.\n\nIn C, programmers use pointers to access data. A pointer is the address of a memory cell. It is easy for programmers to make mistakes when working with pointers, such that a program reads or writes the wrong data. These mistakes can cause programs to crash, misbehave, or allow the program to be taken over by a malicious adversary. Checked C allows programmers to better describe how they intend to use pointers and the range of memory occupied by data that a pointer points to. This information is then used to add checking at runtime to detect mistakes where the wrong data is accessed, instead of the error occurring silently and without detection. This information also can be used detect programming errors while the program is being written. The checking is called \u201cbounds-checking\u201d because it checks that data is being accessed within its intended bounds. The name Checked C reflects the fact that static and dynamic checking is being added to C.\n\nMany programming languages already have bounds checking. C# and Java are examples of such languages. However, those languages automatically add the information needed for bounds checking to data structures. This is a problem for system software, where the programmer needs precise control over what a program is doing. In Checked C, the programmer controls the placement of information needed for bounds-checking and how the information flows through the program, so the programmer retains precise control over what a program is doing.',
 u'Anyone who has used Haskell in a professional setting knows that the String situation is kind of a mess. While in many ways the language is progressing at a rapid pace and is only ever getting more compelling for commercial use, the String situation is still regarded by many people as the largest problem in the langauge. And for good reason, an efficient textual type is absolutely essential for most work and it\u2019s use needs to be streamlined and language-integrated for a overall positive experience writing industrial Haskell.\n\nLet us a consider a logical assessment of why the String Situation exists, how far we can get with workarounds and what\u2019s next. See the accompanying Git project for prototype code:\n\nThe String type is very naive, it\u2019s defined as a linked-list of Char pointers.\n\nThis is not only a bad representation, it\u2019s quite possibly the least efficient (non-contrived) representation of text data possible and has horrible performance in both time and space. And it\u2019s used everywhere in Haskell. Even posterchild libraries for Haskell (Pandoc, etc) use it extensively and have horrible performance because of it.\n\nAround 2005-2007 several more efficient libraries were written, that included Bytestring and Text and both have different use-cases. Both are orders of magnitude more efficient and have become the ubiquitous in \u201cModern Haskell\u201d. Combined with the recent language extension we have a partial solution for routing around the problem.\n\nUnfortunately conversion between the efficient string types and String is \\(O(n)\\) and involves a deep copy. They\u2019re still not used ubiquitously, and every introductory book on the subject still uses String instead of the modern libraries because it\u2019s provided by default.\n\nSo why is String still used? Because it\u2019s too convenient and it has special powers from being wired-in to the compiler.\n\nYou can get pretty far working in a subset of the Prelude and blessed libraries that have nearly removed old historical cruft like String and banished the ugly parts of the Prelude. However one will end up using String in few noticeable dark corners.\n\nOlder core libraries are getting slowly phased out, this is a social problem not a technology problem. This seems to be going in the right direction on it\u2019s own.\n\nFilePaths are not hard to swap out and not a huge concern.\n\ntypeclasses and Pretty printers are the probably the singularly biggest source of continued [Char] usage and what we\u2019ll concern ourselves with here.\n\nThe Show class is really useful, and automatically deriving show much boilerplate is part of the reason Haskell is so much fun to write. However it\u2019s current status poses a bit of a problem transitioning to modern types for several reasons:\n\nSo what is Show class really, it\u2019s so successful that a lot of people actually never look at it\u2019s internals. The guts of it is a function called which is a overloaded CPS\u2019d function which composes together a collection of string fragments for specific implementations of the Show typeclass.\n\nTogether with the Read class we get a textual serializer and deserializer for free with the laws governing that relation being:\n\nGHC can almost always derive this automatically and the instance is pretty simple. Using we can ask GHC to dump it out for us.\n\nThe emergant problem this is that there are an enormous number of pathological Show instances used in practice, and you don\u2019t need to look even beyond the standard library to find law violations. This coupled with the fact that Read instance is really dangerous, it\u2019s use of a very suboptimal String type means that it\u2019s inefficient and opens up security holes and potential denial-of-service attacks in networked applications. Show should really only to be used for debugging the structure of internal types and used at the interactive shell. For serializing structures to text in a way that differs from Haskell\u2019s internal representation we need a pretty printer.\n\nThe correct way of writing custom textual serializes is through the various pretty-print combinator libraries that stem from Wadler\u2019s original paper A prettier printer. There are some degrees of freedom in this design space, but wl-pprint-text is a good choice for almost all use cases. Using the underlying Data.Text.Lazy.Builder functions is also a sensible choice.\n\nSo for example if we have a little \u03bb-calculus\u2026\n\nWe can write down a pretty printer quite simply using the provided combinators. See the gist here for the full example.\n\nSo that\u2019s how it should be done. In practice to do this you\u2019d have to setup a cabal/stack project, install 11 dependencies, write a new typeclass, and write this little joy of a import preamble masking several functions that conflict in the Prelude namespace.\n\nThis kind of sucks. It\u2019s the right thing to do, but it\u2019s kind of painful and it\u2019s certainly not intuitive for newcomers. Abusing Show and String is easier, worst practices should be hard but in this case they are much easier than doing the correct thing.\n\nGHC has had the capacity to support custom Preludes for a while and this is a very wise design choice. For all the historical brokenness of certain things, there are very few technological hurdles to replacing them with modern sensible defaults. The question then remains how close can we come to replacing Show with a Text-based equivalent. The answer is about 80% before surgery is required on GHC itself.\n\nThe translation of a Text-based show prototype is just one module. Instead of concatenating Strings we use the Text.Builder object to build up a Text representation. The function now just becomes a Builder transformation.\n\nThe various builtin types have builder objects implemented by Buildable that efficiently render to Text.\n\nFor constructors with parameters there is a very mechanical translation that is exactly like how works for String.\n\nSince it\u2019s easy to generate the boilerplate instances, we can use Generics to auto-derive the instance for any sum/product type expressible in Haskell and have a DefaultSignature for .\n\nAnd then some ugly (but mechanical) builder munging gives us an exact copy of GHC\u2019s show format. The little known can be used to derive any other class that has an empty minimal set or uses DefaultSignatures and Generic instances to implement methods.\n\nAnd there we have it, a fixed show function that is drop-in compatible with the existing format but uses Text\u2026\n\nWe can even go so far as to tell GHCi to use our custom function at the Repl by adding the following to our projects file.\n\nHowever GHC\u2019s defaulting mechanism has a bunch of ad-hoc specializations for wired-in classes that don\u2019t work for user-defined clases. If we type in an under-specified expression for Show, GHC will just splice in a show dictionary for the unit type if it can\u2019t figure out an appropriate dictionary.\n\nThere\u2019s currently no way to do this for a custom Show type. This implementation also requires a instance and several language extensions. This is the hard limit to how far we can go in \u201cuser space\u201d.\n\nWhat we can prototype with Generics today is not hard to translate over into a builtin deriving mechanism inside the compiler tomorrow. In fact we can create a compatibility layer so close the existing Show class deriving that we reuse all of it\u2019s logic sans the type changes.\n\nRight now in GHC there is a that checks if the derived class is one of the blessed \u201cbuiltins\u201d that has a prescription for deriving a class instance for it. The blessed classes include:\n\nIf the public interface for generating a Text-Show instance recycled the same structure as String version, we could very easily write and plug this into the compiler to derive a new (distinct) text Show that wouldn\u2019t break compatibility.\n\nHowever, at the moment isn\u2019t in GHC\u2019s boot libraries and can\u2019t be made into wired-in type which would be necessary to add the new deriving mechanism to . So that\u2019s as far as we can go in 2016, there\u2019s probably a fairly clear path to removing Stringy-Show if were to at some point become accessible to GHC internals.',
 u'']